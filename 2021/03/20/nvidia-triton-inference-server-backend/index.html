<!DOCTYPE html><html lang="zh-TW" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Triton 後端執行前處理/後處理 | 不務正業工程師的家</title><meta name="description" content="通常在影像相關的模型推論，都會對影像做完前處理後才會丟進模型，以及出來做後處理才會存檔或輸出影像。在前幾篇的範例中，我們將前後處理這兩段放在 Client 端處理，Server 端單純做模型推論，此篇將教學如何將前後處理放在 Server 端，並將整個推論流程串起來。"><meta name="keywords" content="NVIDIA,TRITON INFERENCE SERVER,TRITON SERVER,INFERENCE SERVER, Triton Backend,推論系統,推論,TRITON"><meta name="author" content="Roy Chou"><meta name="copyright" content="Roy Chou"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://roychou121.github.io/2021/03/20/nvidia-triton-inference-server-backend/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="4hHdI8zazPUWtdgydTrGs1GdWfRl5owrCaqeqB416kw"/><meta name="msvalidate.01" content="7C4615B89B83C771BFA3B9AE261D20EE"/><meta property="og:type" content="article"><meta property="og:title" content="Triton 後端執行前處理/後處理"><meta property="og:url" content="https://roychou121.github.io/2021/03/20/nvidia-triton-inference-server-backend/"><meta property="og:site_name" content="不務正業工程師的家"><meta property="og:description" content="通常在影像相關的模型推論，都會對影像做完前處理後才會丟進模型，以及出來做後處理才會存檔或輸出影像。在前幾篇的範例中，我們將前後處理這兩段放在 Client 端處理，Server 端單純做模型推論，此篇將教學如何將前後處理放在 Server 端，並將整個推論流程串起來。"><meta property="og:image" content="https://roychou121.github.io/2021/03/20/nvidia-triton-inference-server-backend/home.png"><meta property="article:published_time" content="2021-03-20T07:00:04.000Z"><meta property="article:modified_time" content="2021-07-14T12:43:02.000Z"><meta name="twitter:card" content="summary"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="透過 MIG-PARTED 管理 MIG" href="https://roychou121.github.io/2021/05/22/nvidia-mig-parted/"><link rel="next" title="Ubuntu 20.04 IPMI 設定" href="https://roychou121.github.io/2021/03/15/ubuntu-ipmitool/"><script async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: 'ca-pub-2690168253582953',
  enable_page_level_ads: 'true'
});</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查詢的內容:${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '複製成功',
    error: '複製錯誤',
    noSupport: '瀏覽器不支援'
  },
  bookmark: {
    message_prev: '按',
    message_next: '鍵將本頁加入書籤'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">載入中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">29</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">標籤</div><div class="length_num">31</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分類</div><div class="length_num">12</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-calendar"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推薦連結</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目錄</div><div class="sidebar-toc__progress"><span class="progress-notice">你已經讀了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#介紹"><span class="toc-number">2.</span> <span class="toc-text">介紹</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#流程"><span class="toc-number">3.</span> <span class="toc-text">流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#範例"><span class="toc-number">4.</span> <span class="toc-text">範例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Server"><span class="toc-number">4.1.</span> <span class="toc-text">Server</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#建立模型庫"><span class="toc-number">4.1.1.</span> <span class="toc-text">建立模型庫</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#編輯後端檔"><span class="toc-number">4.1.2.</span> <span class="toc-text">編輯後端檔</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#編輯設定檔"><span class="toc-number">4.1.3.</span> <span class="toc-text">編輯設定檔</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#運行伺服器端"><span class="toc-number">4.1.4.</span> <span class="toc-text">運行伺服器端</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client"><span class="toc-number">4.2.</span> <span class="toc-text">Client</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#撰寫程式碼"><span class="toc-number">4.2.1.</span> <span class="toc-text">撰寫程式碼</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#運行客戶端"><span class="toc-number">4.2.2.</span> <span class="toc-text">運行客戶端</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/top.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">不務正業工程師的家</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜尋</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-calendar"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推薦連結</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Triton 後端執行前處理/後處理</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="發表於 2021-03-20 15:00:04"><i class="far fa-calendar-alt fa-fw"></i> 發表於 2021-03-20</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新於 2021-07-14 20:43:02"><i class="fas fa-history fa-fw"></i> 更新於 2021-07-14</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta__separator"></i><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/AI/Inference/">Inference</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>閱讀量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="far fa-comments fa-fw post-meta__icon"></i><span>評論數:</span><span class="disqus-comment-count comment-count"><a href="https://roychou121.github.io/2021/03/20/nvidia-triton-inference-server-backend/#disqus_thread"></a></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通常在影像相關的模型推論，都會對影像做完前處理後才會丟進模型，以及出來做後處理才會存檔或輸出影像。在前幾篇的範例中，我們將前後處理這兩段放在 Client 端處理，Server 端單純做模型推論，此篇將教學如何將前後處理放在Server 端，並將整個推論流程串起來。</p>
<p>如還未熟悉 Triton 的可以先看此篇 <a href="https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/">Triton Inference Server 介紹與範例</a>。</p>
<div class="note info">
            <p>其他資料類型的以此類推，本篇將以影像為範例。</p>
          </div>

<hr>
<h2 id="介紹"><a href="#介紹" class="headerlink" title="介紹"></a>介紹</h2><p>在 Trtion v2.4.0 版之後，正式提出了 <a href="https://github.com/triton-inference-server/backend/tree/main" target="_blank" rel="noopener">Triton Backend</a> 功能，允許使用者自行拓展或客製化推理引擎，使得整個框架更加的靈活，目前支援的後端引擎如下列所示，其中 <code>TensorRT</code>、<code>ONNX Runtime</code>、<code>TensorFlow</code> 及 <code>PyTorch</code>，請依上線的模型自行調整，這邊就不再贅述；<code>OpenVINO</code> 是可運行 <a href="https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html" target="_blank" rel="noopener">Intel OpenVINO</a> 的模型；<code>DALI</code> 則是透過 GPU 來加速數據讀取及處理的框架，將於日後做更詳盡的介紹 – <a href="https://roychou121.github.io/2021/06/08/nvidia-dali/">NVIDIA DALI 加速資料載入及處理(未完成)</a>。<br>以及 Trtion v2.11.0 釋出最新的 <a href="https://github.com/triton-inference-server/fil_backend" target="_blank" rel="noopener">FIL Backend</a> 可支援多個機器學習框架（包括 XGBoost、LightGBM、Scikit-Learn 和 cuML）所訓練的森林模型部署。<br>本篇將介紹如後透過 Python Backend 來進行影像的前後處理。</p>
<ul>
<li>TensorRT</li>
<li>ONNX Runtime</li>
<li>TensorFlow</li>
<li>PyTorch</li>
<li>OpenVINO</li>
<li>Python</li>
<li>DALI</li>
<li>FIL (Forest Inference LIbrary)</li>
</ul>
<hr>
<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><img src= "/img/loading.gif" data-src="/2021/03/20/nvidia-triton-inference-server-backend/pythonBackend.jpg" class="" title="流程圖">

<p>需要編寫類別 <code>TritonPythonModel</code>，來處理推論的請求及響應。<br>其中 <code>TritonPythonModel</code> 包含三個 function：<code>initialize</code>、<code>execute</code>、<code>finalize</code>，基本框架如下。</p>
<ul>
<li>initialize : 當模組建立時會被調用，會與設定檔相呼應。</li>
<li>execute : 當 Client 端發出請求時將被調用，這邊就是放置前處理的地方。</li>
<li>finalize : 當模組卸載時會被調用，可放置卸載時需要清除的任何事情。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> triton_python_backend_utils <span class="keyword">as</span> pb_utils</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TritonPythonModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self, args)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(self, requests)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> responses</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">finalize</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="範例"><a href="#範例" class="headerlink" title="範例"></a>範例</h2><p>將用之前<a href="https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/#%E6%92%B0%E5%AF%AB%E7%A8%8B%E5%BC%8F%E7%A2%BC-1">Triton Inference Server 介紹與範例</a>中最後的範例來做示範，mnist 模型訓練及 config 設定照舊，Client 端前處理為下所示，這段將移到 Server 端的 Backend 來處理，這樣Client端的程式只需要傳送及接收影像即可，大幅減輕Client端的負擔。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 前處理</span></span><br><span class="line">img = Image.open(<span class="string">'input.jpg'</span>).convert(<span class="string">'L'</span>)</span><br><span class="line">img = img.resize((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">imgArr = np.asarray(img)/<span class="number">255</span></span><br><span class="line">imgArr = np.expand_dims(imgArr[:, :, np.newaxis], <span class="number">0</span>)</span><br><span class="line">imgArr = imgArr.astype(triton_to_np_dtype(<span class="string">'FP32'</span>))</span><br></pre></td></tr></table></figure>

<h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><h4 id="建立模型庫"><a href="#建立模型庫" class="headerlink" title="建立模型庫"></a>建立模型庫</h4><p>模型庫中保存著多個模型包。<br>除了 mnist 模型之外，後端的模組也是放在這個目錄下，因為多了前處理後端，Client端就不是直接呼叫mnist模型，而是要建立一個 <code>ensemble</code> 來串起前處理及mnist模型</p>
<p>以此範例來說<br>模型庫為 <code>model_repository</code><br>模型庫中共存放了 1 個模型包(<code>mnist</code>)、 1 個 python 後端(<code>preprocess_mnist</code>)及 1 個 ensemble (<code>ensemble_mnist</code>)</p>
<img src= "/img/loading.gif" data-src="/2021/03/20/nvidia-triton-inference-server-backend/repository.PNG" class="" title="建立模型庫">

<h4 id="編輯後端檔"><a href="#編輯後端檔" class="headerlink" title="編輯後端檔"></a>編輯後端檔</h4><p>當 python 前處理後端撰寫完之後，要跟模型一樣寫一個 <code>config.pbtxt</code> ，稍後會有範例說明<br>可以看到 <code>initialize</code> 為撈取 <code>config.pbtxt</code> 參數及 <code>output</code> 變數名命<br><code>execute</code> 為前處理程式碼放置的地方<br><code>finalize</code> 本次未使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> triton_python_backend_utils <span class="keyword">as</span> pb_utils</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TritonPythonModel</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self, args)</span>:</span></span><br><span class="line">        self.model_config = model_config = json.loads(args[<span class="string">'model_config'</span>])</span><br><span class="line"></span><br><span class="line">        output0_config = pb_utils.get_output_config_by_name(model_config, <span class="string">"OUTPUT0"</span>)</span><br><span class="line">        self.output0_dtype = pb_utils.triton_string_to_numpy(output0_config[<span class="string">'data_type'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(self, requests)</span>:</span></span><br><span class="line">        output0_dtype = self.output0_dtype</span><br><span class="line"></span><br><span class="line">        responses = []</span><br><span class="line">        <span class="keyword">for</span> request <span class="keyword">in</span> requests:</span><br><span class="line">            in_0 = pb_utils.get_input_tensor_by_name(request, <span class="string">"INPUT0"</span>)</span><br><span class="line">            </span><br><span class="line">            image = in_0.as_numpy()</span><br><span class="line">            img = Image.open(io.BytesIO(image.tobytes())).convert(<span class="string">'L'</span>)</span><br><span class="line">            img = img.resize((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">            imgArr = np.asarray(img)/<span class="number">255</span></span><br><span class="line">            imgArr = np.expand_dims(imgArr[:, :, np.newaxis], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            out_tensor_0 = pb_utils.Tensor(<span class="string">"OUTPUT0"</span>, imgArr.astype(output0_dtype))</span><br><span class="line"></span><br><span class="line">            inference_response = pb_utils.InferenceResponse(output_tensors=[out_tensor_0])</span><br><span class="line">            responses.append(inference_response)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> responses</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">finalize</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'Cleaning up...'</span>)</span><br></pre></td></tr></table></figure>

<h4 id="編輯設定檔"><a href="#編輯設定檔" class="headerlink" title="編輯設定檔"></a>編輯設定檔</h4><p>mnist 模型設定檔沒動，這邊就沒列出</p>
<p>python 後端設定檔</p>
<figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"preprocess_mnist"</span></span><br><span class="line">backend: <span class="string">"python"</span></span><br><span class="line">max_batch_size: <span class="number">32</span></span><br><span class="line">input [</span><br><span class="line">    &#123;</span><br><span class="line">        name: <span class="string">"INPUT0"</span></span><br><span class="line">        data_type: TYPE_UINT8</span><br><span class="line">        dims: [ -<span class="number">1</span> ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">output [</span><br><span class="line">    &#123;</span><br><span class="line">        name: <span class="string">"OUTPUT0"</span></span><br><span class="line">        data_type: TYPE_FP32</span><br><span class="line">        dims: [<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">instance_group [</span><br><span class="line">    &#123;</span><br><span class="line">        kind: KIND_CPU</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>ensemble設定檔</p>
<figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"ensemble_mnist"</span></span><br><span class="line">platform: <span class="string">"ensemble"</span></span><br><span class="line">max_batch_size: <span class="number">32</span></span><br><span class="line">input [</span><br><span class="line">    &#123;</span><br><span class="line">        name: <span class="string">"INPUT"</span></span><br><span class="line">        data_type: TYPE_UINT8</span><br><span class="line">        dims: [ -<span class="number">1</span> ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">output [</span><br><span class="line">    &#123;</span><br><span class="line">        name: <span class="string">"OUTPUT"</span></span><br><span class="line">        data_type: TYPE_FP32</span><br><span class="line">        dims: [ <span class="number">10</span> ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">ensemble_scheduling &#123;</span><br><span class="line">    step [</span><br><span class="line">        &#123;</span><br><span class="line">            model_name: <span class="string">"preprocess_mnist"</span></span><br><span class="line">            model_version: -<span class="number">1</span></span><br><span class="line">            input_map &#123;</span><br><span class="line">                key: <span class="string">"INPUT0"</span></span><br><span class="line">                value: <span class="string">"INPUT"</span></span><br><span class="line">        &#125;</span><br><span class="line">            output_map &#123;</span><br><span class="line">                key: <span class="string">"OUTPUT0"</span></span><br><span class="line">                value: <span class="string">"preprocessed_image"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">        model_name: <span class="string">"mnist"</span></span><br><span class="line">        model_version: -<span class="number">1</span></span><br><span class="line">        input_map &#123;</span><br><span class="line">            key: <span class="string">"flatten_input"</span></span><br><span class="line">            value: <span class="string">"preprocessed_image"</span></span><br><span class="line">        &#125;</span><br><span class="line">        output_map &#123;</span><br><span class="line">            key: <span class="string">"dense_1"</span></span><br><span class="line">            value: <span class="string">"OUTPUT"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="運行伺服器端"><a href="#運行伺服器端" class="headerlink" title="運行伺服器端"></a>運行伺服器端</h4><p>在執行 <code>tritonserver</code> 之前，如有需要額外安裝套件，可在執行前安裝，如此範例需額外安裝 <code>Pillow</code> 套件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d --gpus 1 --name Triton_Server --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /home/roy/Documents/model_repository/:/models nvcr.io/nvidia/tritonserver:21.09-py3 bash -c "pip install Pillow &amp;&amp;  tritonserver --model-store=/models"</span><br></pre></td></tr></table></figure>

<p>運行伺服器端後，成功的畫面如下所示。</p>
<img src= "/img/loading.gif" data-src="/2021/03/20/nvidia-triton-inference-server-backend/runServer.PNG" class="" title="運行伺服器端成功畫面">

<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><h4 id="撰寫程式碼"><a href="#撰寫程式碼" class="headerlink" title="撰寫程式碼"></a>撰寫程式碼</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tritonclient.grpc <span class="keyword">as</span> grpcclient</span><br><span class="line"><span class="keyword">from</span> tritonclient.utils <span class="keyword">import</span> triton_to_np_dtype</span><br><span class="line"></span><br><span class="line"><span class="comment">## 前處理</span></span><br><span class="line">img = np.fromfile(<span class="string">'input.jpg'</span>, dtype=<span class="string">'uint8'</span>)</span><br><span class="line">image_data = np.expand_dims(img, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Client-Server 溝通</span></span><br><span class="line">triton_client = grpcclient.InferenceServerClient(url=<span class="string">'192.168.137.123:8001'</span>, verbose=<span class="number">0</span>)</span><br><span class="line">inputs = []</span><br><span class="line">inputs.append(grpcclient.InferInput(<span class="string">'INPUT'</span>, image_data.shape, <span class="string">'UINT8'</span>))</span><br><span class="line">inputs[<span class="number">0</span>].set_data_from_numpy(image_data)</span><br><span class="line">outputs = []</span><br><span class="line">outputs.append(grpcclient.InferRequestedOutput(<span class="string">'OUTPUT'</span>,class_count=<span class="number">0</span>))</span><br><span class="line">responses = []</span><br><span class="line">responses.append(triton_client.infer(<span class="string">'ensemble_mnist'</span>,inputs,</span><br><span class="line">                    request_id=str(<span class="number">1</span>),</span><br><span class="line">                    model_version=<span class="string">'1'</span>,</span><br><span class="line">                    outputs=outputs))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 後處理</span></span><br><span class="line"><span class="keyword">print</span> (np.argmax(responses[<span class="number">0</span>].as_numpy(<span class="string">'OUTPUT'</span>)[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<h4 id="運行客戶端"><a href="#運行客戶端" class="headerlink" title="運行客戶端"></a>運行客戶端</h4><p>MNIST 測試資料如下圖所示，將圖片放置掛載的目錄下 (如範例 <code>/raid</code>)。</p>
<img src= "/img/loading.gif" data-src="/2021/03/20/nvidia-triton-inference-server-backend/input.jpg" class="" title="測試資料">

<p>將客戶端環境運行起來後，執行撰寫好的程式碼，即可進行推論。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it --rm --name Triton_Client -v /raid:/data nvcr.io/nvidia/tritonserver:21.09-py3-sdk bash -c 'python /data/client.py'</span><br></pre></td></tr></table></figure>

<p>最後 MNIST 推論結果如下圖所示，可以發現有成功的預測出數字。</p>
<img src= "/img/loading.gif" data-src="/2021/03/20/nvidia-triton-inference-server-backend/result.PNG" class="" title="結果"></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Roy Chou</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章連結: </span><span class="post-copyright-info"><a href="https://roychou121.github.io/2021/03/20/nvidia-triton-inference-server-backend/">https://roychou121.github.io/2021/03/20/nvidia-triton-inference-server-backend/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版權聲明: </span><span class="post-copyright-info">本部落格所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 許可協議。轉載請註明來自 <a href="https://roychou121.github.io" target="_blank">不務正業工程師的家</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NVIDIA/">NVIDIA</a><a class="post-meta__tags" href="/tags/INFERENCE/">INFERENCE</a><a class="post-meta__tags" href="/tags/SDK/">SDK</a></div><div class="post_share"><div class="social-share" data-image="/2021/10/13/ubuntu-reset-password/home.png" data-sites="facebook,linkedin,google,twitter,wechat"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button"><i class="fas fa-qrcode"></i> 打賞<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/img/Bitcoin.png" alt="Bitcoin" onclick="window.open('/img/Bitcoin.png')"/><div class="post-qr-code__desc">Bitcoin</div></li><li class="reward-item"><img class="post-qr-code__img" src="/img/Ethereum.png" alt="Ethereum" onclick="window.open('/img/Ethereum.png')"/><div class="post-qr-code__desc">Ethereum</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/05/22/nvidia-mig-parted/"><img class="prev-cover" data-src="/2021/05/22/nvidia-mig-parted/home.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">透過 MIG-PARTED 管理 MIG</div></div></a></div><div class="next-post pull-right"><a href="/2021/03/15/ubuntu-ipmitool/"><img class="next-cover" data-src="/2021/03/15/ubuntu-ipmitool/home.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Ubuntu 20.04 IPMI 設定</div></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fas fa-comments fa-fw"></i><span> 評論</span></div><div id="disqus_thread"></div><script>var disqus_config = function () {
  this.page.url = 'https://roychou121.github.io/2021/03/20/nvidia-triton-inference-server-backend/';
  this.page.identifier = '2021/03/20/nvidia-triton-inference-server-backend/';
  this.page.title = 'Triton 後端執行前處理/後處理';
};
(function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://roy051023.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script><script>function getDisqusCount() {
  var d = document, s = d.createElement('script');
  s.src = 'https://roy051023.disqus.com/count.js';
  s.id = 'dsq-count-scr';
  (d.head || d.body).appendChild(s);
}

window.addEventListener('load', getDisqusCount, false);</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Roy Chou</div><div class="framework-info"><span>Power by </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="閱讀模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字型"><i class="fas fa-plus"></i></button><button id="font_minus" title="縮小字型"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="簡繁轉換">簡</button></div><div id="rightside-config-show"><button id="rightside_config" title="設定"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直達評論"><i class="scroll_to_comment fas fa-comments"></i></a><button class="close" id="mobile-toc-button" title="目錄"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到頂部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜尋</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜尋文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {
  pangu.autoSpacingPage()
})</script><script src="/js/search/local-search.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>