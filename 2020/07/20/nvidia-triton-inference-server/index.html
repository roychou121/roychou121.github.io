<!DOCTYPE html><html lang="zh-TW" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Triton Inference Server 介紹與範例 | 不務正業工程師的家</title><meta name="description" content="隨著深度學習技術快速的成長，在應用環境中部署和運行 AI 模型的需求也與日俱增。然而開發一個推論解決方案來部署這些模型是一項艱鉅的任務，延遲、吞吐量、支援多個 AI 框架、並行多個模型、GPU 最佳化 ... 等因素，皆是要考慮到的重點，因此如何進行快速部署及管理成為一件複雜卻必須做的事情。"><meta name="keywords" content="NVIDIA,TRITON INFERENCE SERVER,INFERENCE SERVER,推論系統,推論,Triton"><meta name="author" content="Roy Chou"><meta name="copyright" content="Roy Chou"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="4hHdI8zazPUWtdgydTrGs1GdWfRl5owrCaqeqB416kw"/><meta name="msvalidate.01" content="7C4615B89B83C771BFA3B9AE261D20EE"/><meta property="og:type" content="article"><meta property="og:title" content="Triton Inference Server 介紹與範例"><meta property="og:url" content="https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/"><meta property="og:site_name" content="不務正業工程師的家"><meta property="og:description" content="隨著深度學習技術快速的成長，在應用環境中部署和運行 AI 模型的需求也與日俱增。然而開發一個推論解決方案來部署這些模型是一項艱鉅的任務，延遲、吞吐量、支援多個 AI 框架、並行多個模型、GPU 最佳化 ... 等因素，皆是要考慮到的重點，因此如何進行快速部署及管理成為一件複雜卻必須做的事情。"><meta property="og:image" content="https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/home.png"><meta property="article:published_time" content="2020-07-20T06:40:58.000Z"><meta property="article:modified_time" content="2021-01-07T11:40:58.000Z"><meta name="twitter:card" content="summary"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="NVIDIA DIGITS 單機深度學習訓練平台介紹" href="https://roychou121.github.io/2020/07/24/nvidia-digits/"><link rel="next" title="Ubuntu 18.04 安裝 Anaconda3" href="https://roychou121.github.io/2020/07/20/ubuntu-install-anaconda/"><script async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: 'ca-pub-2690168253582953',
  enable_page_level_ads: 'true'
});</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '複製成功',
    error: '複製錯誤',
    noSupport: '瀏覽器不支援'
  },
  bookmark: {
    message_prev: '按',
    message_next: '鍵將本頁加入書籤'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">載入中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">21</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">標籤</div><div class="length_num">28</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分類</div><div class="length_num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推薦連結</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目錄</div><div class="sidebar-toc__progress"><span class="progress-notice">你已經讀了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#介紹"><span class="toc-number">2.</span> <span class="toc-text">介紹</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#特性"><span class="toc-number">2.1.</span> <span class="toc-text">特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#版本"><span class="toc-number">2.2.</span> <span class="toc-text">版本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Server"><span class="toc-number">3.</span> <span class="toc-text">Server</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#系統環境"><span class="toc-number">3.1.</span> <span class="toc-text">系統環境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#安裝步驟"><span class="toc-number">3.2.</span> <span class="toc-text">安裝步驟</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#安裝-Docker"><span class="toc-number">3.2.1.</span> <span class="toc-text">安裝 Docker</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#下載映像檔"><span class="toc-number">3.2.2.</span> <span class="toc-text">下載映像檔</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#運行步驟"><span class="toc-number">3.3.</span> <span class="toc-text">運行步驟</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#模型轉換"><span class="toc-number">3.3.1.</span> <span class="toc-text">模型轉換</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#建立模型庫"><span class="toc-number">3.3.2.</span> <span class="toc-text">建立模型庫</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#編輯設定檔"><span class="toc-number">3.3.3.</span> <span class="toc-text">編輯設定檔</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#運行伺服器端"><span class="toc-number">3.3.4.</span> <span class="toc-text">運行伺服器端</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#檢查系統狀態"><span class="toc-number">3.3.5.</span> <span class="toc-text">檢查系統狀態</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Client"><span class="toc-number">4.</span> <span class="toc-text">Client</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#系統環境-1"><span class="toc-number">4.1.</span> <span class="toc-text">系統環境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#安裝步驟-1"><span class="toc-number">4.2.</span> <span class="toc-text">安裝步驟</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#安裝-Docker-1"><span class="toc-number">4.2.1.</span> <span class="toc-text">安裝 Docker</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#下載映像檔-1"><span class="toc-number">4.2.2.</span> <span class="toc-text">下載映像檔</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#運行步驟-1"><span class="toc-number">4.3.</span> <span class="toc-text">運行步驟</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#串接設備"><span class="toc-number">4.3.1.</span> <span class="toc-text">串接設備</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#撰寫程式碼"><span class="toc-number">4.3.2.</span> <span class="toc-text">撰寫程式碼</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#運行客戶端"><span class="toc-number">4.3.3.</span> <span class="toc-text">運行客戶端</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#範例"><span class="toc-number">5.</span> <span class="toc-text">範例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Server-1"><span class="toc-number">5.1.</span> <span class="toc-text">Server</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#訓練模型"><span class="toc-number">5.1.1.</span> <span class="toc-text">訓練模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#建立模型庫-1"><span class="toc-number">5.1.2.</span> <span class="toc-text">建立模型庫</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#編輯設定檔-1"><span class="toc-number">5.1.3.</span> <span class="toc-text">編輯設定檔</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#運行伺服器端-1"><span class="toc-number">5.1.4.</span> <span class="toc-text">運行伺服器端</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#檢查系統狀態-1"><span class="toc-number">5.1.5.</span> <span class="toc-text">檢查系統狀態</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cleint"><span class="toc-number">5.2.</span> <span class="toc-text">Cleint</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#串接設備-1"><span class="toc-number">5.2.1.</span> <span class="toc-text">串接設備</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#撰寫程式碼-1"><span class="toc-number">5.2.2.</span> <span class="toc-text">撰寫程式碼</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#運行客戶端-1"><span class="toc-number">5.2.3.</span> <span class="toc-text">運行客戶端</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#參考資料"><span class="toc-number">6.</span> <span class="toc-text">參考資料</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/top.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">不務正業工程師的家</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 推薦連結</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Triton Inference Server 介紹與範例</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="發表於 2020-07-20 14:40:58"><i class="far fa-calendar-alt fa-fw"></i> 發表於 2020-07-20</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新於 2021-01-07 19:40:58"><i class="fas fa-history fa-fw"></i> 更新於 2021-01-07</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta__separator"></i><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/AI/Inference/">Inference</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>閱讀量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="far fa-comments fa-fw post-meta__icon"></i><span>評論數:</span><span class="disqus-comment-count comment-count"><a href="https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/#disqus_thread"></a></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><div class="note info">
            <p>2021/01/07 軟體版本更新至 v2.6.0</p>
          </div>

<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>隨著深度學習技術快速的成長，在應用環境中部署和運行 AI 模型的需求也與日俱增。然而開發一個推論解決方案來部署這些模型是一項艱鉅的任務，延遲、吞吐量、支援多個 AI 框架、並行多個模型、GPU 最佳化 … 等因素，皆是要考慮到的重點，因此如何進行快速部署及管理成為一件複雜卻必須做的事情。</p>
<p><a href="https://developer.nvidia.com/nvidia-triton-inference-server" target="_blank" rel="noopener">NVIDIA Triton Inference Server</a> 由 NVIDIA 釋出的一套開源軟體 — 模型推論解決方案，具低延遲、高吞吐等特性，可透過 HTTP 或 GRPC 端點提供客戶端推理服務及服務管理，大幅簡化 AI 模型部署的流程。</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/triton.png" class="" title="Triton Inference Server 架構">

<div class="note info">
            <p>由於 Triton 系統龐大再加上篇幅限制，本章注重在基本功能的操作，較深的功能將於日後配合專案來介紹。</p>
          </div>

<hr>
<h2 id="介紹"><a href="#介紹" class="headerlink" title="介紹"></a>介紹</h2><p>Triton 簡單來看走的是 Client-Server 架構。<br>Server 主要功能為傳接資料，並進行推論；而 Client 則為傳接資料，並結合如網頁、手機 APP 等提升使用者體驗。</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/sc.png" class="" title="Server、Client 示意圖">

<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul>
<li>支援多種 AI 框架<ul>
<li>TensorRT (plan)</li>
<li>ONNX (onnx)</li>
<li>TorchScript (pt)</li>
<li>Tensorflow (graphdef, savedmodel)</li>
</ul>
</li>
<li>支援客製化模組<ul>
<li>Python (py)</li>
<li>DALI (dali)</li>
</ul>
</li>
<li>可同時運行多個模型</li>
<li>可將多個模型串接成一個大模型 (Ensemble Model)</li>
<li>支援 HTTP 和 GRPC 的接口</li>
<li>支援將模型庫放在 Google Cloud Storage 、 Amazon S3 或 Azure Storage 中</li>
<li>具自我監控功能，可顯示 GPU 的利用率，服務器的吞吐量，以及服務器延遲等指標</li>
<li>適用於部署框架 (Kubernetes) </li>
<li>允許使用自定義後端 (C++ 或 Python)</li>
</ul>
<h3 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h3><ul>
<li>2.6.0</li>
</ul>
<div class="note warning">
            <p>此版是由 ubuntu 18.04 轉 ubuntu 20.04 的第一版，部分功能還在轉換中，如有疑慮可先使用 2.5.0 版。</p>
          </div>

<hr>
<h2 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h2><h3 id="系統環境"><a href="#系統環境" class="headerlink" title="系統環境"></a>系統環境</h3><ul>
<li>OS：Ubuntu 20.04</li>
<li>GPU Driver：450.80.02</li>
<li>Docker：19.03.14</li>
<li>Docker Image：nvcr.io/nvidia/tritonserver:20.12-py3</li>
</ul>
<h3 id="安裝步驟"><a href="#安裝步驟" class="headerlink" title="安裝步驟"></a>安裝步驟</h3><h4 id="安裝-Docker"><a href="#安裝-Docker" class="headerlink" title="安裝 Docker"></a>安裝 Docker</h4><p>本篇是將系統架設在 Docker 上，可以參考此<a href="https://roychou121.github.io/2020/07/13/ubuntu-install-docker/">文章</a> 將 Docker 環境建立起來。</p>
<h4 id="下載映像檔"><a href="#下載映像檔" class="headerlink" title="下載映像檔"></a>下載映像檔</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull nvcr.io/nvidia/tritonserver:20.12-py3</span><br></pre></td></tr></table></figure>

<h3 id="運行步驟"><a href="#運行步驟" class="headerlink" title="運行步驟"></a>運行步驟</h3><img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/server.png" class="" title="Server 流程圖">

<h4 id="模型轉換"><a href="#模型轉換" class="headerlink" title="模型轉換"></a>模型轉換</h4><p>系統支援的模型如下，如訓練時所使用的框架非下述所示，則需進行轉換。</p>
<ul>
<li>Tensorflow (graphdef, savedmodel)</li>
<li>TensorRT (plan)</li>
<li>ONNX (onnx)</li>
<li>PyTorch (pt)</li>
</ul>
<div class="note info">
            <p>為了增加系統效能，建議模型都要過 TensorRT 編譯。</p>
          </div>

<h4 id="建立模型庫"><a href="#建立模型庫" class="headerlink" title="建立模型庫"></a>建立模型庫</h4><p>模型庫中保存著多個模型包。<br>每個模型包皆由版本及設定檔所組成，版本建議以自然數依序命名 (Ex. 1,2,3…)。</p>
<p>建立模型庫範例如下圖所示：<br>以此範例來說<br>模型庫為 <code>model-repository</code><br>模型庫中共存放了4個模型包 (<code>mnistTensorrt</code>, <code>mnist-tftrt</code>, <code>tensorflowGraphdef</code>, <code>tensorflowSavedmodel</code>)<br>其中以 <code>mnist-tftrt</code> 模型包為例，<code>mnist-tftrt</code> 為此模型的命名，資料夾中存放了版本 <code>1</code> 的模型及設定檔。</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/repository.png" class="" title="建立模型庫">

<h4 id="編輯設定檔"><a href="#編輯設定檔" class="headerlink" title="編輯設定檔"></a>編輯設定檔</h4><p>每個模型包皆須有設定檔，系統會根據設定檔來運行模型，最基本的設定檔由 5 大元素組成：模型名稱、模型框架、最大批次大小、輸入及輸出層的資訊、GPU 實例資訊。</p>
<p>更詳盡的參數說明可以參考<a href="https://github.com/triton-inference-server/server/blob/master/docs/model_configuration.md" target="_blank" rel="noopener">官方文件</a>。</p>
<p>編輯設定檔範例如下圖所示：</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/config.png" class="" title="編輯設定檔">

<div class="note info">
            <p>設定檔中的模型名稱需與資料夾的模型名稱一致。</p>
          </div>

<div class="note info">
            <p>最大批次大小為推論批次設定的最大值，不一定是實際推論批次大小。</p>
          </div>

<h4 id="運行伺服器端"><a href="#運行伺服器端" class="headerlink" title="運行伺服器端"></a>運行伺服器端</h4><p>在運行前需指定此服務要跑幾個 GPU、走哪些 Port 以及模型庫的路徑。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d --gpus &lt;GPU Number&gt; --name &lt;Conatainer Name&gt; --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p &lt;Host Port&gt;:8000 -p &lt;Host Port&gt;:8001 -p &lt;Host Port&gt;:8002 -v &lt;Model Repository Path&gt;:/models nvcr.io/nvidia/tritonserver:20.12-py3 tritonserver --model-store=/models</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Example</span></span><br><span class="line">sudo docker run -d --gpus all --name Triton_Server --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /home/user/Documents/model-repository:/models nvcr.io/nvidia/tritonserver:20.12-py3 tritonserver --model-store=/models</span><br></pre></td></tr></table></figure>

<div class="note info">
            <p>Port 8000 為 HTTP 協定通道<br>Port 8001 為 GRPC 協定通道</p>
          </div>

<h4 id="檢查系統狀態"><a href="#檢查系統狀態" class="headerlink" title="檢查系統狀態"></a>檢查系統狀態</h4><p>檢查系統是否正常的運行中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -v localhost:8000/v2/health/ready</span><br></pre></td></tr></table></figure>

<p>檢查模型的狀態：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET v2/models[/$&#123;MODEL_NAME&#125;[/versions/$&#123;MODEL_VERSION&#125;]]/stats</span><br></pre></td></tr></table></figure>

<p>Triton 提供 Prometheus Metrics，內容主要為 GPU 狀態和請求統計資訊等：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8002/metrics</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><h3 id="系統環境-1"><a href="#系統環境-1" class="headerlink" title="系統環境"></a>系統環境</h3><ul>
<li>OS：Ubuntu 20.04</li>
<li>Docker：19.03.14</li>
<li>Docker Image：nvcr.io/nvidia/tritonserver:20.12-py3-sdk</li>
</ul>
<h3 id="安裝步驟-1"><a href="#安裝步驟-1" class="headerlink" title="安裝步驟"></a>安裝步驟</h3><h4 id="安裝-Docker-1"><a href="#安裝-Docker-1" class="headerlink" title="安裝 Docker"></a>安裝 Docker</h4><p>本篇是將系統架設在 Docker 上，可以參考此<a href="https://roychou121.github.io/2020/07/13/ubuntu-install-docker/">文章</a> 將 Docker 環境建立起來。</p>
<h4 id="下載映像檔-1"><a href="#下載映像檔-1" class="headerlink" title="下載映像檔"></a>下載映像檔</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull nvcr.io/nvidia/tritonserver:20.12-py3-sdk</span><br></pre></td></tr></table></figure>

<h3 id="運行步驟-1"><a href="#運行步驟-1" class="headerlink" title="運行步驟"></a>運行步驟</h3><img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/client.png" class="" title="Client 流程圖">

<h4 id="串接設備"><a href="#串接設備" class="headerlink" title="串接設備"></a>串接設備</h4><p>目前主流要推論的資料為數據、影像、聲音以及文字，在跟 Server 端溝通之前，Client 端需先跟各設備進行串接。</p>
<p>如影像型推論，就需跟攝影機、機台、儲存設備、呈現平台…等串接。</p>
<h4 id="撰寫程式碼"><a href="#撰寫程式碼" class="headerlink" title="撰寫程式碼"></a>撰寫程式碼</h4><p>Client 端程式碼撰寫主要分成三大部分，依序為</p>
<ul>
<li>前處理<br>此部分主要處裡的事項為資訊進模型前的所有處理，如影像解碼、維度轉換、訊號處理等等。</li>
<li>Client-Server 溝通<br>指定將推論的模型資訊，透過 HTTP 或 GRPC 跟 Server 溝通。<br>官方範例可以參考<a href="https://github.com/triton-inference-server/server/tree/master/src/clients/python/examples" target="_blank" rel="noopener">官方範例</a>。</li>
<li>後處理<br>此部分主要處裡的事項為接到 Server 回傳來推論結果後的所有處理，如儲存檔案、訊息整理、結果呈現等等。</li>
</ul>
<div class="note info">
            <p>前後處理會因不同的應用實例有所差異，此部分不是本章的重點。<br>本章將會著重在說明 Triton Client-Server 基本的溝通函數，其他更深入的函數將在日後介紹。</p>
          </div>


<h4 id="運行客戶端"><a href="#運行客戶端" class="headerlink" title="運行客戶端"></a>運行客戶端</h4><p>在運行前需確認 Server 的 IP、走的協定 (HTTP or GRPC) 以及讀取儲存資料的位置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d --name &lt;Container Name&gt; -v &lt;Data Path&gt;:/data nvcr.io/nvidia/tritonserver:20.12-py3-sdk bash -c '&lt;Client Code&gt;'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Example</span></span><br><span class="line">sudo docker run -d --name Triton_Client -v /home/user/Documents/data:/data nvcr.io/nvidia/tritonserver:20.12-py3-sdk bash -c 'python /data/client.py'</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="範例"><a href="#範例" class="headerlink" title="範例"></a>範例</h2><p>接下來將以大家耳熟能詳的 <a href="https://www.tensorflow.org/quantum/tutorials/mnist" target="_blank" rel="noopener">MNIST</a> 所訓練出的模型當作範例，由於訓練模型不是本篇的重點，可以直接下載訓練好的模型進行後續的練習。</p>
<h3 id="Server-1"><a href="#Server-1" class="headerlink" title="Server"></a>Server</h3><h4 id="訓練模型"><a href="#訓練模型" class="headerlink" title="訓練模型"></a>訓練模型</h4><p>此部分以 MNIST 資料集和簡單的 CNN 模型當作範例。<br>撰寫框架為 TensorFlow v2，來源為<a href="https://www.tensorflow.org/datasets/keras_example" target="_blank" rel="noopener">官方範本</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v2 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"></span><br><span class="line">tf.enable_v2_behavior()</span><br><span class="line"></span><br><span class="line">(ds_train, ds_test), ds_info = tfds.load(</span><br><span class="line">    <span class="string">'mnist'</span>,</span><br><span class="line">    split=[<span class="string">'train'</span>, <span class="string">'test'</span>],</span><br><span class="line">    shuffle_files=<span class="literal">True</span>,</span><br><span class="line">    as_supervised=<span class="literal">True</span>,</span><br><span class="line">    with_info=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_img</span><span class="params">(image, label)</span>:</span></span><br><span class="line">  <span class="string">"""Normalizes images: `uint8` -&gt; `float32`."""</span></span><br><span class="line">  <span class="keyword">return</span> tf.cast(image, tf.float32) / <span class="number">255.</span>, label</span><br><span class="line"></span><br><span class="line">ds_train = ds_train.map(</span><br><span class="line">    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">ds_train = ds_train.cache()</span><br><span class="line">ds_train = ds_train.shuffle(ds_info.splits[<span class="string">'train'</span>].num_examples)</span><br><span class="line">ds_train = ds_train.batch(<span class="number">128</span>)</span><br><span class="line">ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ds_test = ds_test.map(</span><br><span class="line">    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)</span><br><span class="line">ds_test = ds_test.batch(<span class="number">128</span>)</span><br><span class="line">ds_test = ds_test.cache()</span><br><span class="line">ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">  tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">128</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">])</span><br><span class="line">model.compile(</span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(<span class="number">0.001</span>),</span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.fit(</span><br><span class="line">    ds_train,</span><br><span class="line">    epochs=<span class="number">6</span>,</span><br><span class="line">    validation_data=ds_test,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 模型儲存</span></span><br><span class="line">model.save(<span class="string">'model.savedmodel'</span>)</span><br></pre></td></tr></table></figure>

<h4 id="建立模型庫-1"><a href="#建立模型庫-1" class="headerlink" title="建立模型庫"></a>建立模型庫</h4><p>以上面的模型為例，建立出的模型庫如下圖所示。</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/repository_ex.PNG" class="" title="建立模型庫">

<h4 id="編輯設定檔-1"><a href="#編輯設定檔-1" class="headerlink" title="編輯設定檔"></a>編輯設定檔</h4><p>以下的參數為必要欄位，其他深入或客製化的參數，可以參考<a href="https://github.com/triton-inference-server/server/blob/master/docs/model_configuration.md" target="_blank" rel="noopener">官網文件</a>。</p>
<div class="note info">
            <p>使用 <code>gpu_execution_accelerator</code> 可在運行 Triton 時，轉成 TensorRT 來加速。<br>只支援 TensorFlow、ONNX。</p>
          </div>

<figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"mnist"</span></span><br><span class="line">platform: <span class="string">"tensorflow_savedmodel"</span></span><br><span class="line">max_batch_size: <span class="number">32</span></span><br><span class="line">input [</span><br><span class="line">    &#123;</span><br><span class="line">        name: <span class="string">"flatten_input"</span></span><br><span class="line">        data_type: TYPE_FP32</span><br><span class="line">        format: FORMAT_NHWC</span><br><span class="line">        dims: [<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">output [</span><br><span class="line">    &#123;</span><br><span class="line">        name: <span class="string">"dense_1"</span></span><br><span class="line">        data_type: TYPE_FP32</span><br><span class="line">        dims: [<span class="number">10</span>]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">instance_group [</span><br><span class="line">    &#123;</span><br><span class="line">        kind: KIND_GPU</span><br><span class="line">        count: <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">optimization &#123; execution_accelerators &#123;</span><br><span class="line">    gpu_execution_accelerator : [ &#123;</span><br><span class="line">        name : <span class="string">"tensorrt"</span></span><br><span class="line">        parameters &#123; key: <span class="string">"precision_mode"</span> value: <span class="string">"FP16"</span> &#125;&#125;]</span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">version_policy &#123; latest &#123; num_versions: <span class="number">1</span> &#125; &#125;</span><br><span class="line"></span><br><span class="line">dynamic_batching &#123;</span><br><span class="line">  preferred_batch_size: [ <span class="number">4</span>, <span class="number">8</span> ]</span><br><span class="line">  max_queue_delay_microseconds: <span class="number">100</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="運行伺服器端-1"><a href="#運行伺服器端-1" class="headerlink" title="運行伺服器端"></a>運行伺服器端</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d --gpus all --name Triton_Server --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /home/user/Documents/model-repository:/models nvcr.io/nvidia/tritonserver:20.12-py3 tritonserver --model-store=/models --backend-config=tensorflow,version=2 --backend-config=tensorflow,allow-soft-placement=0</span><br></pre></td></tr></table></figure>

<p>運行伺服器端後，成功的畫面如下所示。</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/runServer.PNG" class="" title="運行伺服器端成功畫面">


<h4 id="檢查系統狀態-1"><a href="#檢查系統狀態-1" class="headerlink" title="檢查系統狀態"></a>檢查系統狀態</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -v localhost:8000/v2/health/ready</span><br></pre></td></tr></table></figure>

<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/healthReady.PNG" class="" title="health ready 畫面">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:8000/v2/models/mnist/versions/1/stats</span><br></pre></td></tr></table></figure>

<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/stats.PNG" class="" title="模型狀態">

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8002/metrics</span><br></pre></td></tr></table></figure>

<p>部分 Metrics 畫面：</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/metrics.PNG" class="" title="部分 metrics 畫面">

<h3 id="Cleint"><a href="#Cleint" class="headerlink" title="Cleint"></a>Cleint</h3><h4 id="串接設備-1"><a href="#串接設備-1" class="headerlink" title="串接設備"></a>串接設備</h4><p>由於沒有實際的設備可以串接，此部分直接取用 MNIST 測試資料集中一張影像當作範例，最後將接到的推論結果直接顯示在螢幕上。</p>
<p>MNIST 測試資料如下圖所示。</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/input.jpg" class="" title="測試資料">

<h4 id="撰寫程式碼-1"><a href="#撰寫程式碼-1" class="headerlink" title="撰寫程式碼"></a>撰寫程式碼</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tritonclient.grpc <span class="keyword">as</span> grpcclient</span><br><span class="line"><span class="keyword">from</span> tritonclient.utils <span class="keyword">import</span> triton_to_np_dtype</span><br><span class="line"></span><br><span class="line"><span class="comment">## 前處理</span></span><br><span class="line">img = Image.open(<span class="string">'input.jpg'</span>).convert(<span class="string">'L'</span>)</span><br><span class="line">img = img.resize((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">imgArr = np.asarray(img)/<span class="number">255</span></span><br><span class="line">imgArr = np.expand_dims(imgArr[:, :, np.newaxis], <span class="number">0</span>)</span><br><span class="line">imgArr= imgArr.astype(triton_to_np_dtype(<span class="string">'FP32'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">## Client-Server 溝通</span></span><br><span class="line">triton_client = grpcclient.InferenceServerClient(url=<span class="string">'localhost:8001'</span>, verbose=<span class="number">0</span>)</span><br><span class="line">inputs = []</span><br><span class="line">inputs.append(grpcclient.InferInput(<span class="string">'flatten_input'</span>, imgArr.shape, <span class="string">'FP32'</span>))</span><br><span class="line">inputs[<span class="number">0</span>].set_data_from_numpy(imgArr)</span><br><span class="line">outputs = []</span><br><span class="line">outputs.append(grpcclient.InferRequestedOutput(<span class="string">'dense_1'</span>,class_count=<span class="number">0</span>))</span><br><span class="line">responses = []</span><br><span class="line">responses.append(triton_client.infer(<span class="string">'mnist'</span>,inputs,</span><br><span class="line">                    request_id=str(<span class="number">1</span>),</span><br><span class="line">                    model_version=<span class="string">'1'</span>,</span><br><span class="line">                    outputs=outputs))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 後處理</span></span><br><span class="line"><span class="keyword">print</span> (np.argmax(responses[<span class="number">0</span>].as_numpy(<span class="string">'dense_1'</span>)[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<h4 id="運行客戶端-1"><a href="#運行客戶端-1" class="headerlink" title="運行客戶端"></a>運行客戶端</h4><p>將客戶端環境運行起來後，執行上部撰寫好的程式碼，即可進行推論。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it --rm --name Triton_Client -v /home/user/data:/data nvcr.io/nvidia/tritonserver:20.12-py3-sdk bash -c 'python /data/client.py'</span><br></pre></td></tr></table></figure>

<p>最後 MNIST 推論結果如下圖所示，可以發現有成功的預測出數字。</p>
<img src= "/img/loading.gif" data-src="/2020/07/20/nvidia-triton-inference-server/result.PNG" class="" title="結果">

<hr>
<h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ol>
<li><a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html" target="_blank" rel="noopener">Triton Inference Server Docs</a></li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Roy Chou</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章連結: </span><span class="post-copyright-info"><a href="https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/">https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版權聲明: </span><span class="post-copyright-info">本部落格所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 許可協議。轉載請註明來自 <a href="https://roychou121.github.io" target="_blank">不務正業工程師的家</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NVIDIA/">NVIDIA</a><a class="post-meta__tags" href="/tags/SDK/">SDK</a><a class="post-meta__tags" href="/tags/INFERENCE/">INFERENCE</a></div><div class="post_share"><div class="social-share" data-image="/2021/02/04/ubuntu-network-bond/home.png" data-sites="facebook,linkedin,google,twitter,wechat"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button"><i class="fas fa-qrcode"></i> 打賞<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/img/Bitcoin.png" alt="Bitcoin" onclick="window.open('/img/Bitcoin.png')"/><div class="post-qr-code__desc">Bitcoin</div></li><li class="reward-item"><img class="post-qr-code__img" src="/img/Ethereum.png" alt="Ethereum" onclick="window.open('/img/Ethereum.png')"/><div class="post-qr-code__desc">Ethereum</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/24/nvidia-digits/"><img class="prev-cover" data-src="/2020/07/24/nvidia-digits/home.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NVIDIA DIGITS 單機深度學習訓練平台介紹</div></div></a></div><div class="next-post pull-right"><a href="/2020/07/20/ubuntu-install-anaconda/"><img class="next-cover" data-src="/2020/07/20/ubuntu-install-anaconda/home.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Ubuntu 18.04 安裝 Anaconda3</div></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fas fa-comments fa-fw"></i><span> 評論</span></div><div id="disqus_thread"></div><script>var disqus_config = function () {
  this.page.url = 'https://roychou121.github.io/2020/07/20/nvidia-triton-inference-server/';
  this.page.identifier = '2020/07/20/nvidia-triton-inference-server/';
  this.page.title = 'Triton Inference Server 介紹與範例';
};
(function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://roy051023.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
</script><script>function getDisqusCount() {
  var d = document, s = d.createElement('script');
  s.src = 'https://roy051023.disqus.com/count.js';
  s.id = 'dsq-count-scr';
  (d.head || d.body).appendChild(s);
}

window.addEventListener('load', getDisqusCount, false);</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Roy Chou</div><div class="framework-info"><span>Power by </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="閱讀模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字型"><i class="fas fa-plus"></i></button><button id="font_minus" title="縮小字型"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="簡繁轉換">簡</button></div><div id="rightside-config-show"><button id="rightside_config" title="設定"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直達評論"><i class="scroll_to_comment fas fa-comments"></i></a><button class="close" id="mobile-toc-button" title="目錄"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到頂部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {
  pangu.autoSpacingPage()
})</script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script></body></html>